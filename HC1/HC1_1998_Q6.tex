\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{framed}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{chngpage}
%\usepackage{bigints}

\usepackage{vmargin}
% left top textwidth textheight headheight
% headsep footheight footskip
\setmargins{2.0cm}{2.5cm}{16 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}
\renewcommand{\baselinestretch}{1.3}

\setcounter{MaxMatrixCols}{10}
\begin{document}

\begin{table}[ht!]
     \centering
     \begin{tabular}{|p{15cm}|}
     \hline        
 \noindent \textbf{Part (b)}\\
\noindent 
 The simple linear regression model y = a  + b x + error is often fitted by least squares.  Under what circumstances, which are often assumed, will doing so have conventional optimal properties?
In a simple linear regression analysis the data ( xi, yi), i = 1, …, n are transformed to coded variables Y = y − yo, X = k (x – xo), where xo, yo and k are known constants chosen for convenience of analysis. Given that the fitted regression in coded form is
 X BAY ∧∧∧ +=
with estimated mean squared error S2, obtain the estimates 
∧∧ b,a and mean square
error s2 relating to the original data in terms of 
∧∧ B,A , S2, xo, yo and k.
\\ \hline
 \end{tabular}
\end{table}
\begin{table}[ht!]
     \centering
     \begin{tabular}{|p{15cm}|}
     \hline        
 \noindent \textbf{Part (b)}\\
\noindent (b) The weight of a rabbit is measured at different ages, the results being as shown.
Age (days)  84  91  98 105 112 119 Weight (grams) 527 555 585 615 640 666
Fit a simple linear regression to these data and estimate the residual mean squared error.  Also compute and plot the estimated residuals, and comment on the fit.

\\ \hline
 \end{tabular}
\end{table}


\begin{enumerate}
    \item If the residual(error) terms are i.i.d. $N(0; \sigma^2)$, then the least squares estimates are also maximum
likelihood. Y = y-y0, X = k(x-x0) transforms \[\hat{Y} = \hat{A}+ \hat{B}X\] into \[\hat{y}-y0 = \hat{A}+ \hat{B}k(x-x0)\]
or \[\hat{y} = ( \hat{A} - \hat{B}kx0) + y0 + \hat{B}kx,\] giving in the usual notation 
\[ˆa = y0 + \hat{A} - \hat{B} kx0\] and
\[ \hat{B} = \hat{B} k .\]
Since the scale of Y is not changed, the estimate of $\sigma^2$ will not be changed: s2 = S2.
%%%%%%%%%%%%%%%%5
\item
P
t=15,
P
w=588, n=6,
P
w2=71360,
P
t2=55, using t=(age-84)/7, w=weihgt-500.
P
wt=1960.

\[w -\bar{w} = \hat{B}(t - \bar{t})\] where

\begin{eqnarray*}
\hat{B} &=&
\frac{P(w-\bar{w})(t-\bar{t})}{(t-\bar{t})2}\\
&=& \frac{1960-15\frac{588}{6} }{
55-\frac{15^2}{6} }\\
&=&  \frac{490}{17:5}\\
&=&  28
\end{eqnarray*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Hence w-98=28(t-2.5)=28t-70, or w=28t+28 .
This transforms back to (weight-500)=28
7 (age-84)+28
or weight=500+4(age)-336+28 or weight=4(age)+192 .
The fitted values and residuals are:

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
Age & 84 & 91 & 98 & 105 & 112 & 119\\
Weight & 528 & 556&  584 & 612 & 640&  668\\
Residual & -1 & -1&  1&  3&  0&  -2\\
\end{tabular}
\end{center}


\begin{itemize}
\item sum of squares of residuals =16, hence residual mean square with 4 degrees of freedom is 16/4
=4.
\item The residuals go rather systematically up and then down again, which suggests the need for
a curvilinear model, such as adding a (time)2 term, or plotting log(weight) against log(age).
\end{enumerate}
\end{enumerate}
\end{document}
