\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{framed}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{chngpage}
%\usepackage{bigints}

\usepackage{vmargin}
% left top textwidth textheight headheight
% headsep footheight footskip
\setmargins{2.0cm}{2.5cm}{16 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}
\renewcommand{\baselinestretch}{1.3}

\setcounter{MaxMatrixCols}{10}
\begin{document}
\begin{table}[ht!]
     \centering
     \begin{tabular}{|p{15cm}|}
     \hline        
7. The data $( x_i, y_i)$, where $x_i>0$  and  $i = \{1 ,2, \ldots,n\}$ , are thought to conform to a proportional regression model (linear regression through the origin)
$Y_i = \beta\;x_i + e_i$, $i = \{1 ,2, \ldots,n\}$
where the $e_i$ are independent Normally distributed error terms with mean zero.


(i) If the $e_i$ have constant known variance 
$\sigma^2$, show that the maximum likelihood (ML)
estimate of 
$\beta$
 minimises $ \sum^n_{i=1}(e_i)^2$ and is given by
 
 {
 \large
 \[ \hat{\beta}_{1} = \frac{\sum^n_{i=1}(x_i y_i)}{\sum^n_{i=1}(x_i)^2}  \]
}

\\ \hline
      \end{tabular}
    \end{table}
    

    


\begin{framed}
In the case of simple regression, the formulas for the least squares estimates are 
\[{\displaystyle {\widehat {\beta }}_{1}={\frac {\sum (x_{i}-{\bar {x}})(y_{i}-{\bar {y}})}{\sum (x_{i}-{\bar {x}})^{2}}}{\text{ and }}{\widehat {\beta }}_{0}={\bar {y}}-{\widehat {\beta }}_{1}{\bar {x}}} \]

where 
$ {\displaystyle {\bar {x}}} $
 is the mean (average) of the 
$ {\displaystyle x} $
 values and 
${\displaystyle {\bar {y}}}$ 
 is the mean of the 
$ {\displaystyle y} $
 values.

\end{framed}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{enumerate}[(i)]
\item 7. (i) $Y_i = \beta\;x_i + e_i$ for $i = \{1; 2; \ldots; n\}$, with $\{e_i\}$ i.i.d. $N(0, \sigma^2)$.

Likelihood
\[ L = \prod^n_{i=1} \left[ \frac{1}{\sigma \sqrt{2\pi}} exp\left( - \frac{(y_i - \beta\;x_i)^2}{2\sigma^2}   \right)   \right] \]

\[lnL = \Lambda = -n \ln(\sigma \sqrt{2\pi}) - 1
2\sigma^2\sum^{n}_{i=1}(yi - \beta\;x_i)2.\]

\[
\frac{\partial \Lambda}{\partial \beta} \;=\; 0 + \frac{1}{2 \sigma^2}2 \sum (y_i - \beta\;x_i)x_i\]
1
2\sigma^2
¢ 2
\sum^{n}_{i=1}(y_{i}- \beta\;x_{i})x_{i}\] and is zero when

$\sum (y_{i}- \hat{\beta}\;x_{i})x_{i} = 0$ i.e.

\[\sum y_{i}x_{i} = \beta \sum x_{i}^2\] or $\hat{\beta}1 =
- \frac{\sum y_i\;x_i}{\sum x_i^2}
.$
\[ \frac{\partial^2\Lambda}{\partial^2 \beta} = - \frac{\sum x_i^2}{\sigma^2}, \]

confirming maximum.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
  \begin{table}[ht!]
     \centering
     \begin{tabular}{|p{15cm}|}
     \hline  
(ii) If instead the variance of $e_i$ is given by 
$\sigma^2$
2
x_{i} ,  $i = 1, \ldots, n$,  show that the
ML estimate of 
$\beta$  minimises 

e x i ii n 2 1= ∑ and is given by
ˆ β 2 =   
y x
,
where x and y are the sample mean values of xx n1 ,..., and yy n1 ,..., respectively. 
\\ \hline 
\end{tabular}
\end{table}
\item  If now $\{e_i}$ are $N(0; \sigma2x_{i})$,
Likelihood
\[ L = \prod^n_{i=1} \left[ \frac{1}{\sigma \sqrt{2\pix_i}} exp\left( - \frac{(y_i - \beta\;x_i)^2}{2x_i\sigma^2}   \right)   \right]\]

and\[ \Lambda = -n ln(\sigma
p
\sqrt{2\pi}) ¡ n
2
\sum^{n}_{i=1} \ln x_{i} -
\frac{1}{2\sigma^2} \sum^{n}_{i=1} \frac{(y_{i}- \beta\;x_{i})2}{x_{i}}.
\]

\[\frac{\partial\;\Lambda}{\partial \beta}
= 0 + 0 +
\frac{1}{2\sigma^2}
\sum^{n}_{i=1} \frac{1}{x_{i}}
\frac{(y_{i}- \beta\;x_{i})^2}{x_{i}} =
\frac{1}{\sigma^2} \sum^{n}_{i=1}(y_{i}- \beta\;x_{i}).\]
This is zero when\[
\sum^{n}_{i=1}(y_{i}- \hat{\beta}\;x_{i}) = 0 i.e.
X
yi = \hat{\beta}
X
x_{i}\] or\[ \hat{\beta}^2 =
P
Pyi
x_{i}
.
\]

\[\frac{\partial^2\Lambda}{\partial \beta^2} = \frac{\sum x_i}{2\sigma^2}\] confirming maximum.
\begin{itemize}
\item The first case (i) has $L = Constant - \frac{1}{2\sigma^2} \sum {e^2_i}$
considered as a function of $\beta$;
\item similarly case (ii) has $L = Constant - \frac{1}{2\sigma^2} \sum \frac{e^2_i}{x_{i}}$

\item Considering as a function
of $\beta$. 
\item Thus L is maximized when (i)
P
e2
i or (ii)
P
e2
i =x_{i} is minimized (note
the - sign).
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%
    
  \begin{table}[ht!]
     \centering
     \begin{tabular}{|p{15cm}|}
     \hline  
(iii) Each time a motorcycle is filled with petrol, a record is kept of the amount of petrol in litres (x) used, and the distance travelled in miles (y) since the previous fill-up. Values of x and y recorded on the last 9 occasions were as follows:
\begin{center}
 \begin{tabular}{|c||c|c|c|c|c|c|c|c|c|} \hline 
x & 4.3 & 4.9 & 5.7 & 6.5 & 7.2& 8.3 &8.4 &9.6 &10.1\\ \hline 
y & 123 & 156 & 183 & 183 & 204 & 234& 270 & 273 & 324\\ \hline 
\end{tabular}
   
\end{center}

Plot the data and calculate $\hat{\beta}_1$ and $\hat{\beta}_2$ .  Which of the models (i) or (ii) do you think better represents the data? 
\\ \hline
      \end{tabular}
    \end{table}
\item 
SUM
\begin{center}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|}\hline
X & 4.3& 4.9& 6.5& 5.7& 7.2& 8.3 &8.4 &9.6 &10.1 &  65.0\\ \hline
Y & 123 & 156 & 183 & 183 & 204&  234 & 270& 273 & 324 & 1950\\ \hline
XY & 528.9 & 764.4& 1043.1 & 1189.5& 1468.8& 1942.2 & 2268.0& 2620.8& 3272.4 & 15098.1\\ \hline
X2 & 18.49& 24.01& 32.49& 42.25& 51.84 & 68.89&  70.56& 92.16& 102.01 & 502.70\\ \hline
\end{tabular}
\end{center}


n = 9. ˆ ¯1 = 15098:1
502:7 = 30:034. ˆ ¯2 = 1950
65 = 30:000

%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\begin{itemize}
\item The regression lines are indistinguishable between the two models. 
\item However,
the residuals (difference between y and the value on the line at the same x
- value - i.e. the vertical differences) show a definite tendency to increase as
x increases.
\item For this reason, model (ii) is likely to be better.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\end{enumerate}
\end{document}
