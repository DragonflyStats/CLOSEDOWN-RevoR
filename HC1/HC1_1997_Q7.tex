\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{framed}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{chngpage}
%\usepackage{bigints}

\usepackage{vmargin}
% left top textwidth textheight headheight
% headsep footheight footskip
\setmargins{2.0cm}{2.5cm}{16 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}
\renewcommand{\baselinestretch}{1.3}

\setcounter{MaxMatrixCols}{10}
\begin{document}
\begin{table}[ht!]
     \centering
     \begin{tabular}{|p{15cm}|}
     \hline        
7. The data $( x_i, y_i)$, where x i n i >= 01 , ,..., , are thought to conform to a proportional regression model (linear regression through the origin)
Y x e i n i i i = + = β , ,..., , 1
where the ei are independent Normally distributed error terms with mean zero.
(i) If the ei have constant known variance 
$\sigma^2$, show that the maximum likelihood (ML)
estimate of 
$\beta$
 minimises ei i n 2 1= ∑ and is given by
ˆ β 1 =
xy
x
ii
i
n
i
i
n =
=
∑
∑
1
2
1
.

\\ \hline
      \end{tabular}
    \end{table}
    
  \begin{table}[ht!]
     \centering
     \begin{tabular}{|p{15cm}|}
     \hline  
(ii) If instead the variance of ei is given by 
σ
2
xi ,  i = 1, …, n,  show that the
ML estimate of 
β
 minimises 
e x i ii n 2 1= ∑ and is given by
ˆ β 2 =   
y x
,
where x and y are the sample mean values of xx n1 ,..., and yy n1 ,..., respectively. \\ \hline 
      \end{tabular}
    \end{table}
    
    
  \begin{table}[ht!]
     \centering
     \begin{tabular}{|p{15cm}|}
     \hline  
(iii) Each time a motorcycle is filled with petrol, a record is kept of the amount of petrol in litres (x) used, and the distance travelled in miles (y) since the previous fill-up. Values of x and y recorded on the last 9 occasions were as follows:
\begin{center}
 \begin{tabular}{|c||c|c|c|c|c|c|c|c|c|} \hline 
x & 4.3 & 4.9 & 5.7 & 6.5 & 7.2& 8.3 &8.4 &9.6 &10.1\\ \hline 
y & 123 & 156 & 183 & 183 & 204 & 234& 270 & 273 & 324\\ \hline 
\end{tabular}
   
\end{center}

Plot the data and calculate $\hat{\beta}_1$ and $\hat{\beta}_2$ .  Which of the models (i) or (ii) do you think better represents the data? 
\\ \hline
      \end{tabular}
    \end{table}

\begin{framed}
In the case of simple regression, the formulas for the least squares estimates are 
\[{\displaystyle {\widehat {\beta }}_{1}={\frac {\sum (x_{i}-{\bar {x}})(y_{i}-{\bar {y}})}{\sum (x_{i}-{\bar {x}})^{2}}}{\text{ and }}{\widehat {\beta }}_{0}={\bar {y}}-{\widehat {\beta }}_{1}{\bar {x}}} \]

where 
$ {\displaystyle {\bar {x}}} $
 is the mean (average) of the 
$ {\displaystyle x} $
 values and 
${\displaystyle {\bar {y}}}$ 
 is the mean of the 
$ {\displaystyle y} $
 values.

\end{framed}
\begin{enumerate}[(i)]
\item 7. (i) Yi = ¯xi + ei, i = 1; 2; ¢ ¢ ¢ ; n, feig i.i.d. N(0; ¾2).
\[Likelihood L =
Yn
i=1
f
1
¾
p
2¼
exp[¡
(yi ¡ ¯xi)2
2¾2 ]g,\]
\[lnL = ¤ = ¡n ln(¾
p
2¼) ¡ 1
2¾2
Xn
i=1
(yi ¡ ¯xi)2.\]

\[
\frac{\partial \Lambda}{\partial \beta}

= 0 +
1
2¾2
¢ 2
Xn
i=1
(yi ¡ ¯xi)xi\] and is zero when
P
(yi ¡ ˆ ¯xi)xi = 0 i.e.

\[X
yixi = ˆ ¯
X
xi
2 or ¯ˆ1 =
P
Pyixi
x2
i
.\]
\[@2¤
@¯2 = ¡
P
x2
i
¾2 , confirming maximum.\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item  If now feig are N(0; ¾2xi),
L =
Yn
i=1
f
1
¾
p
2¼xi
exp[¡
(yi ¡ ¯xi)2
2xi¾2 ]g
and ¤ = ¡n ln(¾
p
2¼) ¡ n
2
Xn
i=1
ln xi ¡
1
2¾2
Xn
i=1
(yi ¡ ¯xi)2
xi
.
@¤
@¯
= 0 + 0 +
1
2¾2
¢
Xn
i=1
1
xi
2(yi ¡ ¯xi)xi =
1
¾2
Xn
i=1
(yi ¡ ¯xi).
This is zero when
X
(yi ¡ ˆ ¯xi) = 0 i.e.
X
yi = ˆ ¯
X
xi or ¯ˆ2 =
P
Pyi
xi
.
6
@2¤
@¯2 = ¡
P
xi
¾2 , confirming maximum.
\begin{itemize}
\item The first case (i) has L = Constant¡ 1
2¾2
P
e2
i , considered as a function of ¯;
\item similarly case (ii) has L = Constant ¡ 1
2¾2
P e2
i
xi
. 
\item Considering as a function
of ¯. 
\item Thus L is maximized when (i)
P
e2
i or (ii)
P
e2
i =xi is minimized (note
the - sign).
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%
\item 
SUM
\begin{center}
\begin{tabular}{c|cccc|c|c|c|c|c|c|}
X & 4.3& 4.9& 6.5& 5.7& 7.2& 8.3 &8.4 &9.6 &10.1 &  65.0\\
Y & 123 & 156 & 183 & 183 & 204&  234 & 270& 273 & 324 & 1950\\
XY & 528.9 & 764.4& 1043.1 & 1189.5& 1468.8& 1942.2 & 2268.0& 2620.8& 3272.4 & 15098.1\\
X2 & 18.49& 24.01& 32.49& 42.25& 51.84 & 68.89&  70.56& 92.16& 102.01 & 502.70\\
\end{tabular}
\end{center}


n = 9. ˆ ¯1 = 15098:1
502:7 = 30:034. ˆ ¯2 = 1950
65 = 30:000

%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\begin{itemize}
\item The regression lines are indistinguishable between the two models. 
\item However,
the residuals (difference between y and the value on the line at the same x
- value - i.e. the vertical differences) show a definite tendency to increase as
x increases.
\item For this reason, model (ii) is likely to be better.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\end{enumerate}
\end{document}
