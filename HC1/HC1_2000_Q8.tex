\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{framed}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{chngpage}
%\usepackage{bigints}

\usepackage{vmargin}
% left top textwidth textheight headheight
% headsep footheight footskip
\setmargins{2.0cm}{2.5cm}{16 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}
\renewcommand{\baselinestretch}{1.3}

\setcounter{MaxMatrixCols}{10}
\begin{document}\begin{table}[ht!]
     \centering
     \begin{tabular}{|p{15cm}|}
     \hline        
The accompanying edited Minitab output shows some regression analysis of the progress in sales in 100s (y) of hi-tec widgets over time in months (x).  Use the output to answer the following questions. 
 
(i) In the output for Regression 1, the p-values for the t tests for the slope and intercept parameters are missing.  Use available information to test the fitted parameters for statistical significance.  Also use the output to calculate corr(x, y).  Comment critically on the adequacy of this regression as a model for the data, having regard to Plots 1A and 1B. (5) 

 
 \\ \hline
      \end{tabular}
    \end{table}
    


\begin{enumerate}
    \item The t-value is given as 2.70, and residual d.f.=7, so p-value is 2P(t7 > 2:70).
    \begin{itemize}
        \item From tables,P(t7 > 2:70)
:=
0:0153.
    \end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}
    \item The (2-tail) p-value therefore is about 0.031 for intercept(
interpolation between p=0.05 and 0.01 in a suitable table would give the same
answer).
\item For the slope, the t-value is 10.48,so P < 0:001, corr(x; y) =
p
R2 =
p
0:940 = 0:97

%%%%%%%%%%%%%%%%%%%%
\item From plot1A,the linear relation may be breaking down above about x=8. 
%%%%%%%%%%%%%%%%%%%%
\item Plot 1B supports
the inadequacy of a linear model for the full set of data, because the residuals do not appear to be a normally distribution set with mean zero.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
  \begin{table}[ht!]
     \centering
     \begin{tabular}{|p{15cm}|}
     \hline  
 
(ii) Contrast the significance of the x term in Regression 2 with the result of your test for the x term in Regression 1.  Interpret the statement ‘R-Sq = 98.1\%’ in terms of the Analysis of Variance output for Regression 2.  What standard assumption(s) about the distribution of the error term may be called into question by Plot 2?  What other reason is there for not accepting Regression 2 as an adequate model for the data? (5) 
  \\ \hline
      \end{tabular}
    \end{table}
    
\item When x2 is included, x ceases to be significant.
\begin{itemize}
    \item The information in x2 is clearly
taking up that previously given by x (with which it is strongly correlated).
\item Now the model
explain 98.1\% of the total variation among the y-values.
\item Residual variance may increase with x;whereas it should be constant .There appears to
be a non-random pattern of residual.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item Plot 3A shows that the logarithmic trend explains data better. Plot 3B is more
like a random scatter of residual(though still a bit suspect)
\begin{itemize}
\item The amount of total variation
in the explain by regression 3 is 99.1\% the test of any of these models.
\item The total sum of squares = (n ¡ 1) £ variance of y;the units of y in regression 3 are
logarithmic, whereas in 1 and 2 they were natural.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55
  \begin{table}[ht!]
     \centering
     \begin{tabular}{|p{15cm}|}
     \hline  
 
Critically compare Regressions 1 and 3 for their success in fitting the data.  Interpret the statement ‘R-Sq = 99.1\%’ in Regression 3 in terms of the Analysis of Variance output, and explain why the total sum of squares in Regression 3 (0.28760) differs from the total sum of squares in Regressions 1 and 2 (186120). 
  \\ \hline
      \end{tabular}
    \end{table}
    
  \begin{table}[ht!]
     \centering
     \begin{tabular}{|p{15cm}|}
     \hline  
 Use each of the three regressions to give point estimates of sales at 10 months after launching the product.  State with reasons which of the three estimates you think is the best. (5)   
 \\ \hline 
      \end{tabular}
    \end{table}
\item When x=10
1 gives y = 78:33 + 540
:=
618
2 gives y = 170 + 40 + 500
:=
710
3 gives log10y = 2:16 + 0:689 = 2:849; i:e: y
:=
706
\begin{itemize}
    \item However data do not go so far as x=10 and it is only in regression 3 that we have a
model that seems at all safe to use for extrapolation beyond the range of available data.
\item On all counts (see (iii) and(iv)) Regression 3 appears best, and also it requires only two
parameters.
\end{itemize}

\end{enumerate}
\newpage
\begin{framed}
\begin{verbatim}
MTB > Print ’y’ 175  195  225  275  333  383  423  483  643 
# sales of widgets (100s)   
MTB > Print ’x’   1    2    3    4    5    6    7    8    9 

# time from product launch (months)   MTB > Plot ’y’ ’x’.                     640+                                                         *                y                                        480+                                                  *                    PLOT 1A            -                                                                       ------            -                                            *            -                                     *                     320+                              *            -                        *                        -                 *            -          *         160+    *              ------+---------+---------+---------+---------+---------+x                  1.5       3.0       4.5       6.0       7.5       9.0
 \end{verbatim}
 \end{framed}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{framed}
 \begin{verbatm}
 MTB >  #################   REGRESSION  1   #################   
 MTB > Regress ’y’ 1 ’x’; 
 SUBC>  Residuals C4.  # C4 stores residuals from the regression of y on x   The regression equation is y = 78.3 + 54.0 x   Predictor        Coef       StDev          T        P   Constant        78.33       29.01       2.70   x              54.000       5.155      10.48   S = 39.93       R-Sq = 94.0%     R-Sq(adj) = 93.1%   Analysis of Variance   Source            DF          SS          MS         F        P   Regression         1      174960      174960    109.74    0.000   Residual Error     7       11160        1594   Total              8      186120   MTB >  Let C5 = 78.33 + 54*’x’     # C5 gives the fitted values from regression 1   MTB > Name  C3 ’x-sq’  C4 ’slr-res’ C5 ’slr-fit’   MTB > Plot ’slr-res’ ’slr-fit’.          80+                                                        *                slr-res                                   40+  *                                                                    PLOT 1B            -                                                                       ------                                    -        *           0+                        -               *      *      *     *            -                                                 *            -                                          *         -40+                          ----+---------+---------+---------+---------+---------+--slr-fit                160       240       320       400       480       560
   MTB > Let C3 = ’x’*’x’       # C3 is time  squared   
   
  \end{verbatim}
 \end{framed}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{framed}
 \begin{verbatm}  
   MTB >  #################   REGRESSION  2   #################   MTB > Regress ’y’ 2  ’x’ ’x-sq’; SUBC>   Residuals C6.   The regression equation is   y = 170 + 4.0 x + 5.00 x-sq   Predictor        Coef       StDev          T        P   Constant       170.00       30.56       5.56    0.001   x                4.00       14.03       0.29    0.785   x-sq            5.000       1.368       3.65    0.011   S = 24.01       R-Sq = 98.1%     R-Sq(adj) = 97.5%   Analysis of Variance   Source            DF          SS          MS         F        P   Regression         2      182660       91330    158.38    0.000   Residual Error     6        3460         577   Total              8      186120   Source       DF      Seq SS   x             1      174960   x-sq          1        7700   MTB > Let C7=170 + 4*’x’ + 5*’x-sq’    #  C7 stores fitted values from regression 2   MTB > Name C6 ’quadrres’ C7 ’quadrfit’
 
 
12 
   MTB > Plot ’quadrres’ ’quadrfit’.            -                                                       *          25+            -                  *    quadrres-                                                                       PLOT 2            -            *             *                                            -----                       0+       *            - *  *                                    -                                  *         -25+                                    -                                            *                          --------+---------+---------+---------+---------+--------quadrfit                    240       320       400       480       560
   MTB > Let C8 = logten(’y’)      # C8 contains log (y) to the base 10   MTB > Name C8 ’log10(y)’   
   MTB > Plot ’log10(y)’ ’x’.                    2.80+                                                         *                log10(y)            -                                                  *            -                                            *        2.60+                                     *                                 PLOT 3A            -                                                                       ------            -                              *                        -                        *        2.40+            -                 *                        -          *            -    *        2.20+              ------+---------+---------+---------+---------+---------+x                  1.5       3.0       4.5       6.0       7.5       9.0
   
 \end{verbatim}
 \end{framed}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{framed}
 \begin{verbatm}
   MTB >  #################   REGRESSION  3   #################   MTB > Regress ’log10(y)’ 1  ’x’; SUBC>   Residuals C9.   # C9 stores residuals from regression 3   The regression equation is   log10(y) = 2.16 + 0.0689 x   
   
   
   Predictor        Coef       StDev          T        P   Constant      2.16086     0.01422     151.93    0.000   x            0.068910    0.002527      27.26    0.000   S = 0.01958     R-Sq = 99.1%     R-Sq(adj) = 98.9%   Analysis of Variance   Source            DF          SS          MS         F        P   Regression         1     0.28492     0.28492    743.35    0.000   Residual Error     7     0.00268     0.00038   Total              8     0.28760   
 \end{verbatim}
 \end{framed}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{framed}
 \begin{verbatm}   
   MTB > Let C10=2.16086 + 0.06891*’x’   # C10 stores fitted values from regression 3   
   MTB > Name C9 ’logy-res’  C10 ’logy-fit’   
   MTB > Plot ’logy-res’ ’logy-fit’.    logy-res            -                                                     *                   0.020+            -                              *            -       *                                                               PLOT 3B            -                                    *                                  ------            -                        *       0.000+                        -             *                        -                  *                      *      -0.020+                        -                                               *                          +---------+---------+---------+---------+---------+------logy-fit           2.16      2.28      2.40      2.52      2.64      2.76    
\end{verbatim}
\end{framed}
\end{document}

x <- 1:9
y <- c(175,195,225,275,333,383,423,483,643) 
df <- data.frame(x,y)
