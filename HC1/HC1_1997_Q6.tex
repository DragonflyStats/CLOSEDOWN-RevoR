\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{framed}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{chngpage}
%\usepackage{bigints}

\usepackage{vmargin}
% left top textwidth textheight headheight
% headsep footheight footskip
\setmargins{2.0cm}{2.5cm}{16 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}
\renewcommand{\baselinestretch}{1.3}

\setcounter{MaxMatrixCols}{10}
\begin{document}



\section{Introduction}
\begin{enumerate}[(i)]
\item (i) P(X · ») =
Z »
0
dx
$\mu$
= »
$\mu$
, for 0 · » · $\mu$; = 0 for » < 0; = 1 for » > $\mu$.

\[E(X) = \int^{\theta}_{0} \frac{x\;dx}{\theta} = \frac{1}{2\theta}\left[ x^2 \right]^{\theta}_{0} = \frac{\theta}{2}\]



\[E(X^2) = \int^{\theta}_{0} \frac{x^2\;dx}{\theta} = \frac{1}{3\theta}\left[ x^3 \right]^{\theta}_{0} = \frac{\theta^2}{3}\]

\[ Var(X) = [E(X^2)] - [E(X)^2] = \frac{\theta^2}{3} - \frac{\theta^2}{4} = \frac{\theta^2}{12}\]
%%%%%%%%%%%%%%%%%%
\item Sampled items chosen at random, hence fXig are independent. 
\begin{itemize}
\item P(X · x) =
x
$\mu$ for each item, all are required to be · x, so probability for n items is (x
$\mu$ )n,
when 0 · x · $\mu$. 
\item This is F(X(n)), where X(n) is the sample maximum, and
so f(x(n)) = F
0(x(n)) = nxn¡1
$\mu$n , when 0 · x · $\mu$, =0 otherwise.
\item E[X(n)] =
Z $\mu$
0
nxn
$\mu$n dx = [ nxn+1
(n + 1)$\mu$n ]$\mu$
0 = n$\mu$
n + 1
,
\item E[X2
(n)] =
Z $\mu$
0
nxn+1
$\mu$n dx = [ nxn+2
(n + 2)$\mu$n ]$\mu$
0 = n$\mu$2
n + 2
.

\end{itemize}
Hence
V [X(n)] = n$\mu$2
n+2 ¡ ( n$\mu$
n+1)2 = $\mu$2f n
n+2 ¡ n2
(n+1)2 g
= $\mu$2
(n+1)2(n+2)fn(n + 1)2 ¡ n2(n + 2)g = n$\mu$2
(n+1)2(n+2) :
\begin{itemize}
\item n+1
n X(n) is unbiased for estimating $\mu$.
\item V ar[n+1
n X(n)] = (n+1
n )2 n$\mu$2
(n+1)2(n+2) = $\mu$2
n(n+2) .
\item The likelihood of a sample fx1; ¢ ¢ ¢ ; xng is 1
$\mu$n , (0 · x · $\mu$).
\item Setting ˆ$\mu$ = X(n), where is the lowest value of $\mu$ possible on the evidence of
the sample values, gives the largest possible value of the likelihood.
\item  Hence X(n) is the m.l. estimator.
\end{itemize}
%%%%%%%%%%%%%%%%%%
\item Method of moments estimator ˜$\mu$ is found from setting ¯x = E[x], i.e., ¯x = 1
2
˜$\mu$
so that ˜$\mu$ = 2¯x. V [˜$\mu$] = 4V [¯x] = 4
nV [x] = 4
n ¢ $\mu$2
12 = $\mu$2=3n.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
% 1997 Question 6

\begin{eqnarray*}
V(\hat{\theta}) &=& 4 \times V(\bar{x})\\
 &=& \frac{4}{n} \times V(x) \\
 &=& \frac{4}{n} \times \frac{\theta^2}{12} \\
 &=& \frac{\theta^2}{3n}
\end{eqnarray*}

%%%%%%%%%%%%%%%%%%
\item n+1
n X(n) is unbiased, and X(n) very nearly so if n is at all large; their
variances are much smaller than that for ˜$\mu$, the estimator based on the mean.
Hence, use X(n) if there are a reasonable number of offcuts; if only few,
multiply by the factor n+1
n .
\end{enumerate}
\end{document}