\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{framed}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{chngpage}
%\usepackage{bigints}

\usepackage{vmargin}
% left top textwidth textheight headheight
% headsep footheight footskip
\setmargins{2.0cm}{2.5cm}{16 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}
\renewcommand{\baselinestretch}{1.3}

\setcounter{MaxMatrixCols}{10}
\begin{document}
\begin{enumerate}
\item  The Central Limit Theorem says that if fxig are n independent observations,
all taken from the same distribution with (finite) mean and variance ¹ and
¾2, then the limiting distribution as n ! 1 of the mean $\bar{x}$ is N(¹; ¾2=n).

\begin{itemize}
\item Therefore if ¹; ¾2 are known, the actual form of the distribution of fxig is
not important provided n is large. 
\item However, if the X distribution is skew,
n in practice needs to be very large, 500+, whereas if this distribution is
symmetrical, even though not itself normal, a sample of less than 50 observations
may be adequate for the the approximation to be used.
\item It is the basis
for “large sample” tests of means and differences of means; and can also be
applied to discrete distributions, e.g. in testing proportions.
\end{itemize}

\item  If there are systematic differences between “blocks” or groups of items which
have to be used in the same experiment, as well as random variation and
systematic effects of treatments, a linear model (as in 2(a)) needs to contain
a term for blocks: yij = ¹ + ¿i + ¯j + eij , (i = 1 to v; j = 1 to r), where
every treatment, 1 to v, appears once in every block, 1 to r. A “two-way”
analysis of variance, to remove blocks as well as treatments from the total
sum of squares, is required:
SOURCE OF VARIATION D.F.
Blocks r ¡ 1
Treatments v ¡ 1
Residual (r ¡ 1)(v ¡ 1)
TOTAL rv ¡ 1
13
The feijg are required to be i.i.d. N(0; ¾2), and an estimate of ¾2 is provided
by the residual mean square in this analysis.
\item  Degrees of freedom is a parameter in a Â2-distribution. The square of a
N(0; 1) variate is X2
(1), and the sum of the squares of n independent N(0; 1)0s
is Â2(
n). Thus for X = N(¹; ¾2), so that X¡¹
¾ = r is N(0; 1) the sum of n
independent observations
Xn
i=1
(xi ¡ ¹
¾
)2 is Â2
(n). When ¹ is not known and
must be replaced by an estimate $\bar{x}$ from a sample,
Xn
i=1
(xi ¡ $\bar{x}$
¾
)2 is Â2(
n¡1)
because the n values (xi ¡ $\bar{x}$) used in the calculation have one constraint
placed on them, namely
Xn
i=1
(xi ¡ $\bar{x}$) = 0 by definition of $\bar{x}$. 

\begin{itemize}
\item In other applications
of Â2, such as tests in contingency tables or tests of goodness of fit, the rule for degrees of freedom is “number of independent items of information minus number of constraints placed on them”. 
\item Constraints include fixing the
marginal totals and the grand total in tables when calculating expected values, and having to estimate parameters from the observed data (e.g. mean in a Poisson distribution) before expected values can be calculated. 
\item In the analysis of variance, d.f. for total are N ¡ 1 when N observations are available, v ¡1 for v treatments etc. 
\item The t-statistic always has the same number
of d.f. as the estimate s2 used in it, e.g. the residual d.f. in the analysis of variance.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item  Confidence intervals often give more information than significance tests
based on the same sample of data. When a parameter (e.g. ¹) has been
estimated (by, e.g. $\bar{x}$) we may set up a Null Hypothesis that ¹ takes a
certain value and then test whether $\bar{x}$ is close enough to this for the NH not
to be rejected. However, there is a whole range of values of ¹ which would
be consistent with the observed value of $\bar{x}$, and it is this range which forms
a confidence interval. Fox example, if fxig are taken from N(¹; ¾2), with ¾2
known, and their mean is $\bar{x}$, then we know that if i = 1 to n then $\bar{x}$ has the
distribution N(¹; ¾2=n), so that P(¡1:96 · $\bar{x}$¡¹
¾=
p
n
· +1:96) = 0:95, which
can be written as P($\bar{x}$ ¡ 1:96¾=
p
n · ¹ · $\bar{x}$ + 1:96¾=
p
n) = 0:95, giving a
95% confidence interval for the true value of ¹, based on the sample mean
$\bar{x}$. With probability 0.95, the interval contains ¹. To alter the probability,
or confidence level, to e.g. 90% or 99% the corresponding r-values (N(0; 1))
must be used instead of 1.96, e.g. 1.645 and 2.576. Confidence intervals can
be set up whenever we know the distribution of a parameter estimate, e.g.
14
s2 for ¾2, b for ¯ in linear regression. If an interval is wide relative to the size
of the estimate, the lack of precision is immediately clear; this information
is hidden in a significance test.
\end{document}